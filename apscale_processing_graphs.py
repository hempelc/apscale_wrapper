#!/usr/bin/env python3

"""
A script to generate processing graphs for apscale runs.

Requirements: apscale project directory that contains a Project_report.xlsx file
with sheets named 3_PE merging, 4_primer_trimming, and 5_quality_filtering,
as well as the folders 4_primer_trimming, 7_otu_clustering, 8_denoising, and 9_lulu_filtering with all result files.

Credit: some graphs are modified versions of the graphs generated by the apscale GUI

By Chris Hempel (christopher.hempel@kaust.edu.sa) on Jun 7 2023
"""

import os
import datetime
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import plotly.colors as pc
import argparse
import warnings
import dash_bio
import subprocess
from statistics import mean, median, stdev
import time
import requests_html
import aiohttp
import asyncio
from aiohttp_retry import RetryClient, ExponentialRetry
from tqdm.asyncio import tqdm


# Define that warnings are not printed to console
warnings.filterwarnings("ignore")


# Funtion to print datetime and text
def time_print(text):
    print(datetime.datetime.now().strftime("%H:%M:%S"), ": ", text, sep="")


# Function for statistics calculation
def calculate_read_stats(lst):
    return [
        min(lst),
        max(lst),
        round(mean(lst), 2),
        round(median(lst), 2),
        round(stdev(lst), 2),
    ]


# Function to replace all but leftmost duplicates in row with NA
def replace_duplicates_with_nan(df):
    columns = df.columns[::-1]
    for i in range(len(columns) - 1):
        mask = df[columns[i]] == df[columns[i + 1]]
        df.loc[mask, columns[i]] = np.nan
    return df


# Function to format a taxonomy df for krona
def krona_formatting(df):
    if database_format == "bold":
        ranks = ["phylum", "class", "order", "family", "genus", "species"]
    else:
        ranks = ["domain", "phylum", "class", "order", "family", "genus", "species"]
    # Sum samples
    sample_sums = df.drop(
        columns=["ID", "Seq", "lowest_taxon", "lowest_rank", "total_reads"] + ranks
    ).sum(axis=1)
    krona_df = pd.concat([sample_sums.rename("Sum"), df[ranks]], axis=1)
    # Fix taxonomy formatting
    ## Turn all non-taxa names into NaN
    krona_df = krona_df.replace(
        {
            "Taxonomy unreliable - multiple matching taxa": np.nan,
            "Taxonomy unreliable - percentage similarity threshold for rank not met": np.nan,
            "Taxonomy unreliable - bitscore and alignment length threshold not met": np.nan,
            "Taxonomy unreliable - confidence threshold not met": np.nan,
            "Not available in database": np.nan,
            "Unknown in PR2 database": np.nan,
            "Unknown in BOLD database": np.nan,
            "Unknown in SILVA database": np.nan,
        }
    ).replace("_", " ", regex=True)
    ## If entire taxonomy is NaN, replace with "Taxonomy unreliable"
    for index, row in krona_df.iterrows():
        if database_format == "bold":
            if pd.isna(row["phylum"]):
                krona_df.loc[index, "phylum":] = "Taxonomy unreliable"
        elif pd.isna(row["domain"]):
            krona_df.loc[index, "domain":] = "Taxonomy unreliable"
    ## Fill NaNs with last tax entry
    krona_df = krona_df.fillna(method="ffill", axis=1)

    # Aggregate taxa
    krona_df_agg = krona_df.groupby(ranks)["Sum"].sum().reset_index()
    # Move the Sum column to the front
    krona_df_agg = krona_df_agg[["Sum"] + ranks]
    # Replace all but leftmost duplicates in row with NA and return
    return replace_duplicates_with_nan(krona_df_agg[krona_df_agg["Sum"] > 0])


# Define a function to standardize species names based on GBIF taxonomy
def gbif_parent_check(phylum_name, species_name):
    """
    Standardizes species against the GBIF API (when in doubt based on phylum).

    Returns:
        str: The standardized species name or None if no match.
    """
    time.sleep(0.1)
    with requests_html.HTMLSession() as session:
        request_name = "%20".join(species_name.split(" "))
        response = session.get(
            f"https://api.gbif.org/v1/species/match?verbose=true&name={request_name}&limit=1"
        )
        api_response_json = response.json()

        if (
            "note" in api_response_json
            and "Multiple equal matches" in api_response_json["note"]
        ):
            for match in api_response_json.get("alternatives", []):
                if phylum_name == match.get("phylum", None):
                    return match.get("species", None)
        return api_response_json.get("species", None)


# Define a wrapper function for the standardization of species names based on GBIF taxonomy
def gbif_check_taxonomy(df, unit):
    taxon_table_df = df[["phylum", "species"]]
    # Define excpetions that are no real taxon names and drop them in the df. Also only keep unique species
    exceptions = [
        "Taxonomy unreliable - multiple matching taxa",
        "Taxonomy unreliable - percentage similarity threshold for rank not met",
        "Taxonomy unreliable - bitscore and alignment length threshold not met",
        "No match in database",
        "Unknown in PR2 database",
        "Unknown in BOLD database",
        "Unknown in SILVA database",
        "Unknown in MIDORI2 database",
        "Taxonomy unreliable - confidence threshold not met",
        "No match in database",
    ]
    taxon_table_df = taxon_table_df.replace(exceptions, None).dropna().drop_duplicates()

    # Check if the 'species' column only contains None values, and if it does, exit early
    if taxon_table_df["species"].isnull().all():
        time_print(f"No valid species found among {unit}s. Skipping map generation.")
        return []

    checked_species = []
    # Standardize names
    for _, row in taxon_table_df.iterrows():
        phylum_name = row["phylum"]
        species_name = row["species"]
        if checked_species_name := gbif_parent_check(phylum_name, species_name):
            checked_species.append(checked_species_name)
    # Dropcontamination species
    contamination_species = [
        "Sus scrofa",
        "Bos taurus",
        "Homo sapiens",
        "Gallus gallus",
        "Canis lupus",
        "Felis catus",
        "Ovis aries",
        "Capra hircus",
    ]
    return [taxon for taxon in checked_species if taxon not in contamination_species]


# Function to calculate overlap between two rows (to sort the continent df)
def calculate_overlap(row1, row2):
    return np.sum(row1 & row2)


# Define functions to download GBIF specimen locations asynchronously
## Custom exception for handling HTTP 503 errors
class HTTP503Error(Exception):
    pass


## Custom exception for handling SSL errors
class SSLConnectionError(Exception):
    pass


class ServerDisconnectedError(Exception):
    pass


timeout = aiohttp.ClientTimeout(total=120, connect=20, sock_connect=20, sock_read=40)
retry_options = ExponentialRetry(
    attempts=5,
    exceptions={
        HTTP503Error,
        SSLConnectionError,
        ServerDisconnectedError,
    },  # Retry on 3 known error
)


async def fetch_occurrences(retry_session, taxon_name, country_code):
    request_name = "%20".join(taxon_name.split(" "))
    url = f"https://api.gbif.org/v1/occurrence/search?scientificName={request_name}&country={country_code}"
    try:
        async with retry_session.get(url, timeout=timeout) as response:
            if response.status == 200:
                try:
                    api_response_json = await response.json()
                    return api_response_json.get("count", 0)
                except aiohttp.ContentTypeError:
                    time_print(f"Unexpected content type at URL: {url}")
                    return 0
            elif response.status == 503:
                raise HTTP503Error(
                    f"Service unavailable for {taxon_name} in {country_code}"
                )
            else:
                time_print(
                    f"Error fetching data for {taxon_name} in {country_code}: HTTP {response.status}"
                )
                return 0
    except aiohttp.client_exceptions.ServerDisconnectedError:
        raise ServerDisconnectedError(
            f"Server connection error for {taxon_name} in {country_code}"
        )
    except aiohttp.client_exceptions.ClientOSError:
        raise SSLConnectionError(
            f"SSL connection error for {taxon_name} in {country_code}"
        )


async def fetch_all_occurrences(retry_session, taxon_name, country_codes):
    tasks = [
        fetch_occurrences(retry_session, taxon_name, country_code)
        for country_code in country_codes
    ]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # Process each result to handle exceptions individually
    final_results = []
    for result, country_code in zip(results, country_codes):
        if isinstance(result, HTTP503Error):
            time_print(
                f"Service unavailable for {taxon_name} in {country_code}. Setting counts to 0."
            )
            final_results.append(0)  # Default value for unavailable data
        elif isinstance(result, SSLConnectionError):
            time_print(
                f"SSL connection error for {taxon_name} in {country_code}. Setting counts to 0."
            )
            final_results.append(0)  # Default value for SSL errors
        elif isinstance(result, ServerDisconnectedError):
            time_print(
                f"Server disconnected for {taxon_name} in {country_code}. Setting counts to 0."
            )
            final_results.append(0)  # Default value for server disconnects
        else:
            final_results.append(result)
    return final_results


async def async_main(gbif_standardized_species, country_codes, occurrence_df):
    async with aiohttp.ClientSession() as session:
        async with RetryClient(session, retry_options=retry_options) as retry_session:
            for taxon_name in tqdm(
                gbif_standardized_species, desc="Downloading GBIF species location data"
            ):
                occurrence_list = await fetch_all_occurrences(
                    retry_session, taxon_name, country_codes
                )
                occurrence_df[taxon_name] = occurrence_list
    return occurrence_df


def maps_and_continent_plot_generation(gbif_standardized_species_list, unit):
    """
    Returns:
        species_maps, continent_occurrence_plot
    """

    # Return empty dictionary and None for the plots if the species list is empty, effectively skipping this step
    if not gbif_standardized_species_list:
        return {}, None

    time_print(
        f"Generating GBIF maps, continent occurrence plot, and realm occurrence plot for {unit}s.."
    )

    # Define a dictionary with all countries and codes on Earth
    country_codes_dict = {
        "Andorra": ["AD", "Europe", "Palearctic"],
        "United Arab Emirates": ["AE", "Asia", "Palearctic"],
        "Afghanistan": ["AF", "Asia", "Palearctic"],
        "Antigua and Barbuda": ["AG", "North America", "Neotropical"],
        "Anguilla": ["AI", "North America", "Neotropical"],
        "Albania": ["AL", "Europe", "Palearctic"],
        "Armenia": ["AM", "Asia", "Palearctic"],
        "Angola": ["AO", "Africa", "Afrotropical"],
        "Antarctica": ["AQ", "Antarctica", "Antarctic"],
        "Argentina": ["AR", "South America", "Neotropical"],
        "American Samoa": ["AS", "Oceania", "Oceanian"],
        "Austria": ["AT", "Europe", "Palearctic"],
        "Australia": ["AU", "Oceania", "Australasian"],
        "Aruba": ["AW", "North America", "Neotropical"],
        "Åland Islands": ["AX", "Europe", "Palearctic"],
        "Azerbaijan": ["AZ", "Asia", "Palearctic"],
        "Bosnia and Herzegovina": ["BA", "Europe", "Palearctic"],
        "Barbados": ["BB", "North America", "Neotropical"],
        "Bangladesh": ["BD", "Asia", "Indomalayan"],
        "Belgium": ["BE", "Europe", "Palearctic"],
        "Burkina Faso": ["BF", "Africa", "Afrotropical"],
        "Bulgaria": ["BG", "Europe", "Palearctic"],
        "Bahrain": ["BH", "Asia", "Palearctic"],
        "Burundi": ["BI", "Africa", "Afrotropical"],
        "Benin": ["BJ", "Africa", "Afrotropical"],
        "Saint Barthélemy": ["BL", "North America", "Neotropical"],
        "Bermuda": ["BM", "North America", "Neotropical"],
        "Brunei Darussalam": ["BN", "Asia", "Indomalayan"],
        "Bolivia": ["BO", "South America", "Neotropical"],
        "Bonaire, Sint Eustatius and Saba": ["BQ", "North America", "Neotropical"],
        "Brazil": ["BR", "South America", "Neotropical"],
        "Bahamas": ["BS", "North America", "Neotropical"],
        "Bhutan": ["BT", "Asia", "Indomalayan"],
        "Bouvet Island": ["BV", "Antarctica", "Antarctic"],
        "Botswana": ["BW", "Africa", "Afrotropical"],
        "Belarus": ["BY", "Europe", "Palearctic"],
        "Belize": ["BZ", "North America", "Neotropical"],
        "Canada": ["CA", "North America", "Nearctic"],
        "Cocos (Keeling) Islands": ["CC", "Asia", "Indomalayan"],
        "Congo (Democratic Republic)": ["CD", "Africa", "Afrotropical"],
        "Central African Republic": ["CF", "Africa", "Afrotropical"],
        "Congo": ["CG", "Africa", "Afrotropical"],
        "Switzerland": ["CH", "Europe", "Palearctic"],
        "Côte d'Ivoire": ["CI", "Africa", "Afrotropical"],
        "Cook Islands": ["CK", "Oceania", "Oceanian"],
        "Chile": ["CL", "South America", "Neotropical"],
        "Cameroon": ["CM", "Africa", "Afrotropical"],
        "China": ["CN", "Asia", "Palearctic/Indomalayan"],
        "Colombia": ["CO", "South America", "Neotropical"],
        "Costa Rica": ["CR", "North America", "Neotropical"],
        "Cuba": ["CU", "North America", "Neotropical"],
        "Cabo Verde": ["CV", "Africa", "Afrotropical"],
        "Curaçao": ["CW", "North America", "Neotropical"],
        "Christmas Island": ["CX", "Asia", "Indomalayan"],
        "Cyprus": ["CY", "Asia", "Palearctic"],
        "Czechia": ["CZ", "Europe", "Palearctic"],
        "Germany": ["DE", "Europe", "Palearctic"],
        "Djibouti": ["DJ", "Africa", "Afrotropical"],
        "Denmark": ["DK", "Europe", "Palearctic"],
        "Dominica": ["DM", "North America", "Neotropical"],
        "Dominican Republic": ["DO", "North America", "Neotropical"],
        "Algeria": ["DZ", "Africa", "Palearctic"],
        "Ecuador": ["EC", "South America", "Neotropical"],
        "Estonia": ["EE", "Europe", "Palearctic"],
        "Egypt": ["EG", "Africa", "Palearctic"],
        "Western Sahara": ["EH", "Africa", "Palearctic"],
        "Eritrea": ["ER", "Africa", "Afrotropical"],
        "Spain": ["ES", "Europe", "Palearctic"],
        "Ethiopia": ["ET", "Africa", "Afrotropical"],
        "Finland": ["FI", "Europe", "Palearctic"],
        "Fiji": ["FJ", "Oceania", "Australasian"],
        "Falkland Islands": ["FK", "South America", "Neotropical"],
        "Micronesia": ["FM", "Oceania", "Oceanian"],
        "Faroe Islands": ["FO", "Europe", "Palearctic"],
        "France": ["FR", "Europe", "Palearctic"],
        "Gabon": ["GA", "Africa", "Afrotropical"],
        "United Kingdom": ["GB", "Europe", "Palearctic"],
        "Grenada": ["GD", "North America", "Neotropical"],
        "Georgia": ["GE", "Asia", "Palearctic"],
        "French Guiana": ["GF", "South America", "Neotropical"],
        "Guernsey": ["GG", "Europe", "Palearctic"],
        "Ghana": ["GH", "Africa", "Afrotropical"],
        "Gibraltar": ["GI", "Europe", "Palearctic"],
        "Greenland": ["GL", "North America", "Nearctic"],
        "Gambia": ["GM", "Africa", "Afrotropical"],
        "Guinea": ["GN", "Africa", "Afrotropical"],
        "Guadeloupe": ["GP", "North America", "Neotropical"],
        "Equatorial Guinea": ["GQ", "Africa", "Afrotropical"],
        "Greece": ["GR", "Europe", "Palearctic"],
        "South Georgia and the South Sandwich Islands": [
            "GS",
            "Antarctica",
            "Antarctic",
        ],
        "Guatemala": ["GT", "North America", "Neotropical"],
        "Guam": ["GU", "Oceania", "Oceanian"],
        "Guinea-Bissau": ["GW", "Africa", "Afrotropical"],
        "Guyana": ["GY", "South America", "Neotropical"],
        "Hong Kong": ["HK", "Asia", "Indomalayan"],
        "Heard Island and McDonald Islands": ["HM", "Antarctica", "Antarctic"],
        "Honduras": ["HN", "North America", "Neotropical"],
        "Croatia": ["HR", "Europe", "Palearctic"],
        "Haiti": ["HT", "North America", "Neotropical"],
        "Hungary": ["HU", "Europe", "Palearctic"],
        "Indonesia": ["ID", "Asia", "Indomalayan"],
        "Ireland": ["IE", "Europe", "Palearctic"],
        "Israel": ["IL", "Asia", "Palearctic"],
        "Isle of Man": ["IM", "Europe", "Palearctic"],
        "India": ["IN", "Asia", "Indomalayan"],
        "British Indian Ocean Territory": ["IO", "Asia", "Indomalayan"],
        "Iraq": ["IQ", "Asia", "Palearctic"],
        "Iran": ["IR", "Asia", "Palearctic"],
        "Iceland": ["IS", "Europe", "Palearctic"],
        "Italy": ["IT", "Europe", "Palearctic"],
        "Jersey": ["JE", "Europe", "Palearctic"],
        "Jamaica": ["JM", "North America", "Neotropical"],
        "Jordan": ["JO", "Asia", "Palearctic"],
        "Japan": ["JP", "Asia", "Palearctic"],
        "Kenya": ["KE", "Africa", "Afrotropical"],
        "Kyrgyzstan": ["KG", "Asia", "Palearctic"],
        "Cambodia": ["KH", "Asia", "Indomalayan"],
        "Kiribati": ["KI", "Oceania", "Oceanian"],
        "Comoros": ["KM", "Africa", "Afrotropical"],
        "Saint Kitts and Nevis": ["KN", "North America", "Neotropical"],
        "Korea (Democratic People's Republic)": ["KP", "Asia", "Palearctic"],
        "Korea (Republic)": ["KR", "Asia", "Palearctic"],
        "Kuwait": ["KW", "Asia", "Palearctic"],
        "Cayman Islands": ["KY", "North America", "Neotropical"],
        "Kazakhstan": ["KZ", "Asia", "Palearctic"],
        "Lao People's Democratic Republic": ["LA", "Asia", "Indomalayan"],
        "Lebanon": ["LB", "Asia", "Palearctic"],
        "Saint Lucia": ["LC", "North America", "Neotropical"],
        "Liechtenstein": ["LI", "Europe", "Palearctic"],
        "Sri Lanka": ["LK", "Asia", "Indomalayan"],
        "Liberia": ["LR", "Africa", "Afrotropical"],
        "Lesotho": ["LS", "Africa", "Afrotropical"],
        "Lithuania": ["LT", "Europe", "Palearctic"],
        "Luxembourg": ["LU", "Europe", "Palearctic"],
        "Latvia": ["LV", "Europe", "Palearctic"],
        "Libya": ["LY", "Africa", "Palearctic"],
        "Morocco": ["MA", "Africa", "Palearctic"],
        "Monaco": ["MC", "Europe", "Palearctic"],
        "Moldova (the Republic of)": ["MD", "Europe", "Palearctic"],
        "Montenegro": ["ME", "Europe", "Palearctic"],
        "Saint Martin (French part)": ["MF", "North America", "Neotropical"],
        "Madagascar": ["MG", "Africa", "Afrotropical"],
        "Marshall Islands": ["MH", "Oceania", "Oceanian"],
        "Republic of North Macedonia": ["MK", "Europe", "Palearctic"],
        "Mali": ["ML", "Africa", "Afrotropical"],
        "Myanmar": ["MM", "Asia", "Indomalayan"],
        "Mongolia": ["MN", "Asia", "Palearctic"],
        "Macao": ["MO", "Asia", "Indomalayan"],
        "Northern Mariana Islands": ["MP", "Oceania", "Oceanian"],
        "Martinique": ["MQ", "North America", "Neotropical"],
        "Mauritania": ["MR", "Africa", "Palearctic"],
        "Montserrat": ["MS", "North America", "Neotropical"],
        "Malta": ["MT", "Europe", "Palearctic"],
        "Mauritius": ["MU", "Africa", "Afrotropical"],
        "Maldives": ["MV", "Asia", "Indomalayan"],
        "Malawi": ["MW", "Africa", "Afrotropical"],
        "Mexico": ["MX", "North America", "Nearctic/Neotropical"],
        "Malaysia": ["MY", "Asia", "Indomalayan"],
        "Mozambique": ["MZ", "Africa", "Afrotropical"],
        "Namibia": ["NA", "Africa", "Afrotropical"],
        "New Caledonia": ["NC", "Oceania", "Australasian"],
        "Niger": ["NE", "Africa", "Afrotropical"],
        "Norfolk Island": ["NF", "Oceania", "Australasian"],
        "Nigeria": ["NG", "Africa", "Afrotropical"],
        "Nicaragua": ["NI", "North America", "Neotropical"],
        "Netherlands": ["NL", "Europe", "Palearctic"],
        "Norway": ["NO", "Europe", "Palearctic"],
        "Nepal": ["NP", "Asia", "Indomalayan"],
        "Nauru": ["NR", "Oceania", "Oceanian"],
        "Niue": ["NU", "Oceania", "Oceanian"],
        "New Zealand": ["NZ", "Oceania", "Australasian"],
        "Oman": ["OM", "Asia", "Palearctic"],
        "Panama": ["PA", "North America", "Neotropical"],
        "Peru": ["PE", "South America", "Neotropical"],
        "French Polynesia": ["PF", "Oceania", "Oceanian"],
        "Papua New Guinea": ["PG", "Oceania", "Australasian"],
        "Philippines": ["PH", "Asia", "Indomalayan"],
        "Pakistan": ["PK", "Asia", "Palearctic"],
        "Poland": ["PL", "Europe", "Palearctic"],
        "Saint Pierre and Miquelon": ["PM", "North America", "Nearctic"],
        "Pitcairn": ["PN", "Oceania", "Oceanian"],
        "Puerto Rico": ["PR", "North America", "Neotropical"],
        "Palestine, State of": ["PS", "Asia", "Palearctic"],
        "Portugal": ["PT", "Europe", "Palearctic"],
        "Palau": ["PW", "Oceania", "Oceanian"],
        "Paraguay": ["PY", "South America", "Neotropical"],
        "Qatar": ["QA", "Asia", "Palearctic"],
        "Réunion": ["RE", "Africa", "Afrotropical"],
        "Romania": ["RO", "Europe", "Palearctic"],
        "Serbia": ["RS", "Europe", "Palearctic"],
        "Russian Federation": ["RU", "Europe/Asia", "Palearctic"],
        "Rwanda": ["RW", "Africa", "Afrotropical"],
        "Saudi Arabia": ["SA", "Asia", "Palearctic"],
        "Solomon Islands": ["SB", "Oceania", "Australasian"],
        "Seychelles": ["SC", "Africa", "Afrotropical"],
        "Sudan": ["SD", "Africa", "Afrotropical"],
        "Sweden": ["SE", "Europe", "Palearctic"],
        "Singapore": ["SG", "Asia", "Indomalayan"],
        "Saint Helena, Ascension and Tristan da Cunha": [
            "SH",
            "Africa",
            "Afrotropical",
        ],
        "Slovenia": ["SI", "Europe", "Palearctic"],
        "Svalbard and Jan Mayen": ["SJ", "Europe", "Palearctic"],
        "Slovakia": ["SK", "Europe", "Palearctic"],
        "Sierra Leone": ["SL", "Africa", "Afrotropical"],
        "San Marino": ["SM", "Europe", "Palearctic"],
        "Senegal": ["SN", "Africa", "Afrotropical"],
        "Somalia": ["SO", "Africa", "Afrotropical"],
        "Suriname": ["SR", "South America", "Neotropical"],
        "South Sudan": ["SS", "Africa", "Afrotropical"],
        "Sao Tome and Principe": ["ST", "Africa", "Afrotropical"],
        "El Salvador": ["SV", "North America", "Neotropical"],
        "Syrian Arab Republic": ["SY", "Asia", "Palearctic"],
        "Eswatini": ["SZ", "Africa", "Afrotropical"],
        "Turks and Caicos Islands": ["TC", "North America", "Neotropical"],
        "Chad": ["TD", "Africa", "Afrotropical"],
        "French Southern Territories": ["TF", "Antarctica", "Antarctic"],
        "Togo": ["TG", "Africa", "Afrotropical"],
        "Thailand": ["TH", "Asia", "Indomalayan"],
        "Tajikistan": ["TJ", "Asia", "Palearctic"],
        "Tokelau": ["TK", "Oceania", "Oceanian"],
        "Timor-Leste": ["TL", "Asia", "Indomalayan"],
        "Turkmenistan": ["TM", "Asia", "Palearctic"],
        "Tunisia": ["TN", "Africa", "Palearctic"],
        "Tonga": ["TO", "Oceania", "Oceanian"],
        "Turkey": ["TR", "Asia", "Palearctic"],
        "Trinidad and Tobago": ["TT", "North America", "Neotropical"],
        "Tuvalu": ["TV", "Oceania", "Oceanian"],
        "Taiwan": ["TW", "Asia", "Indomalayan"],
        "Tanzania": ["TZ", "Africa", "Afrotropical"],
        "Ukraine": ["UA", "Europe", "Palearctic"],
        "Uganda": ["UG", "Africa", "Afrotropical"],
        "United States Minor Outlying Islands": ["UM", "Oceania", "Oceanian"],
        "United States of America": ["US", "North America", "Nearctic"],
        "Uruguay": ["UY", "South America", "Neotropical"],
        "Uzbekistan": ["UZ", "Asia", "Palearctic"],
        "Holy See": ["VA", "Europe", "Palearctic"],
        "Saint Vincent and the Grenadines": ["VC", "North America", "Neotropical"],
        "Venezuela (Bolivarian Republic of)": ["VE", "South America", "Neotropical"],
        "Virgin Islands (British)": ["VG", "North America", "Neotropical"],
        "Virgin Islands (U.S.)": ["VI", "North America", "Neotropical"],
        "Viet Nam": ["VN", "Asia", "Indomalayan"],
        "Vanuatu": ["VU", "Oceania", "Oceanian"],
        "Wallis and Futuna": ["WF", "Oceania", "Oceanian"],
        "Samoa": ["WS", "Oceania", "Oceanian"],
        "Yemen": ["YE", "Asia", "Palearctic"],
        "Mayotte": ["YT", "Africa", "Afrotropical"],
        "South Africa": ["ZA", "Africa", "Afrotropical"],
        "Zambia": ["ZM", "Africa", "Afrotropical"],
        "Zimbabwe": ["ZW", "Africa", "Afrotropical"],
    }

    # Extract country codes from the dictionary keys
    country_codes = [values[0] for values in country_codes_dict.values()]

    # Make a df template
    occurrence_df = pd.DataFrame(list(country_codes_dict), columns=["Country"])

    # Run the asynchronous GBIF specimen location retrieval function
    asyncio.run(
        async_main(gbif_standardized_species_list, country_codes, occurrence_df)
    )

    num_viridis_colors = 10
    viridis_colors = pc.sequential.Viridis[:num_viridis_colors]
    # Calculate the interval step for the remaining colors
    interval_step = 1 / (num_viridis_colors - 1)
    custom_colors = [[0, "#d3d3d3"], [0.00000001, viridis_colors[0]]]
    # Append the rest of the Viridis colors at even intervals
    for i in range(1, num_viridis_colors):
        interval_position = i * interval_step
        custom_colors.append([interval_position, viridis_colors[i]])
    # Ensure the last color is exactly at position 1
    custom_colors[-1][0] = 1

    # Generate a map per species
    species_maps = {}
    for species in gbif_standardized_species_list:
        # Create the map for specimen counts
        map_gbif_specimen_counts = px.choropleth(
            occurrence_df,
            locations="Country",
            locationmode="country names",
            hover_name="Country",
            hover_data={
                species: True,
                "Country": False,
            },
            color=species,
            scope="world",
            color_continuous_scale=custom_colors,
        )
        map_gbif_specimen_counts.update_layout(
            title_text=f"{species} - GBIF specimen count by country",
            geo=dict(
                showframe=False,
            ),
            coloraxis_colorbar=dict(
                title="GBIF specimen count",
            ),
        )
        map_gbif_specimen_counts.update_traces(
            hovertemplate="<b>%{hovertext}</b><br>GBIF specimen count: %{customdata[0]}"
        )
        species_maps[species] = map_gbif_specimen_counts

    # Generate continent occurrence plot
    ## Generate binary occurrence data per continent and species
    occurrence_df["Continent"] = occurrence_df["Country"].map(
        lambda x: country_codes_dict.get(x, [None, None])[1]
    )
    ### Handle cases in which a country falls in 2 continents (make multiple rows)
    occurrence_df["Continent"] = occurrence_df["Continent"].str.split("/")
    occurrence_df = occurrence_df.explode("Continent")
    continent_df = occurrence_df.drop("Country", axis=1).groupby("Continent").sum()
    continent_df[continent_df > 0] = 1

    ## Sort the df to minimize gaps
    continent_df = continent_df.T
    ### Start with the row with the most 1s
    sorted_continent_df = (
        continent_df.loc[continent_df.sum(axis=1).idxmax()].to_frame().T
    )
    remaining_df = continent_df.drop(sorted_continent_df.index)

    while not remaining_df.empty:
        last_row = sorted_continent_df.iloc[-1]
        ### Calculate overlap with the last row in sorted_continent_df
        overlaps = remaining_df.apply(
            lambda row: calculate_overlap(last_row, row), axis=1
        )
        ### Select the row with the maximum overlap
        next_row_idx = overlaps.idxmax()
        next_row = remaining_df.loc[next_row_idx]
        ### Append the selected row to the sorted DataFrame
        sorted_continent_df = pd.concat([sorted_continent_df, next_row.to_frame().T])
        ### Remove the selected row from the remaining rows
        remaining_df = remaining_df.drop(next_row_idx)

    sorted_continent_df = sorted_continent_df.T

    sorted_continent_df = (
        sorted_continent_df.reset_index()
        .rename(columns={"index": "Continent"})
        .iloc[:, ::-1]
    )

    ## Melt the DataFrame to long format
    continent_df_melted = sorted_continent_df.melt(
        id_vars=["Continent"], var_name="Species", value_name="Detected"
    )

    ## Define a minimum plot height
    row_num = len(continent_df_melted["Species"].unique())
    plot_height = 480 if row_num < 16 else 30 * row_num

    ## Generate the bubble plot using Plotly
    continent_occurrence_plot = px.scatter(
        continent_df_melted,
        x="Continent",
        y="Species",
        size="Detected",
        color="Continent",
        hover_name="Continent",
        size_max=10,
        title=f"Detected species by continent - {unit}s",
        height=plot_height,
        width=550,
    )
    continent_occurrence_plot.update_xaxes(tickangle=35)

    # Generate realm occurrence plot
    ## Generate binary occurrence data per continent and species
    occurrence_df["Biogeographic realm"] = occurrence_df["Country"].map(
        lambda x: country_codes_dict.get(x, [None, None])[2]
    )
    ### Handle cases in which a country falls in 2 realms (make multiple rows)
    occurrence_df["Biogeographic realm"] = occurrence_df[
        "Biogeographic realm"
    ].str.split("/")
    occurrence_df = occurrence_df.explode("Biogeographic realm")
    realm_df = (
        occurrence_df.drop("Country", axis=1)
        .drop("Continent", axis=1)
        .groupby("Biogeographic realm")
        .sum()
    )
    realm_df[realm_df > 0] = 1

    ## Sort the df to minimize gaps
    realm_df = realm_df.T
    ### Start with the row with the most 1s
    sorted_realm_df = realm_df.loc[realm_df.sum(axis=1).idxmax()].to_frame().T
    remaining_df = realm_df.drop(sorted_realm_df.index)

    while not remaining_df.empty:
        last_row = sorted_realm_df.iloc[-1]
        ### Calculate overlap with the last row in sorted_realm_df
        overlaps = remaining_df.apply(
            lambda row: calculate_overlap(last_row, row), axis=1
        )
        ### Select the row with the maximum overlap
        next_row_idx = overlaps.idxmax()
        next_row = remaining_df.loc[next_row_idx]
        ### Append the selected row to the sorted DataFrame
        sorted_realm_df = pd.concat([sorted_realm_df, next_row.to_frame().T])
        ### Remove the selected row from the remaining rows
        remaining_df = remaining_df.drop(next_row_idx)

    sorted_realm_df = sorted_realm_df.T

    sorted_realm_df = (
        sorted_realm_df.reset_index()
        .rename(columns={"index": "Biogeographic realm"})
        .iloc[:, ::-1]
    )

    ## Melt the DataFrame to long format
    realm_df_melted = sorted_realm_df.melt(
        id_vars=["Biogeographic realm"], var_name="Species", value_name="Detected"
    )

    ## Define a minimum plot height
    row_num = len(realm_df_melted["Species"].unique())
    plot_height = 480 if row_num < 16 else 30 * row_num

    ## Generate the bubble plot using Plotly
    realm_occurrence_plot = px.scatter(
        realm_df_melted,
        x="Biogeographic realm",
        y="Species",
        size="Detected",
        color="Biogeographic realm",
        hover_name="Biogeographic realm",
        size_max=10,
        title=f"Detected species by biogeographic realm - {unit}s",
        height=plot_height,
        width=650,
    )
    realm_occurrence_plot.update_xaxes(tickangle=35)

    return species_maps, continent_occurrence_plot, realm_occurrence_plot


# Define a custom validation function for the parameters
def validate_args(args):
    if args.add_taxonomy == "True" and not args.database_format:
        parser.error("--database_format is required when --add_taxonomy=True.")


# Define arguments
parser = argparse.ArgumentParser(
    description="A script to generate processing graphs for apscale runs.",
)
parser.add_argument(
    "--project_dir",
    help="Directory containing apscale results to generate reports for.",
    required=True,
)
parser.add_argument(
    "--graph_format",
    help="Graph format, either HTML, png, svg. HTML is recommended because it creates interactive plots (default: html).",
    default="html",
    choices=["png", "svg", "html"],
)
parser.add_argument(
    "--min_length",
    metavar="NNN",
    type=int,
    help="Minimum limit of expected amplicon length (will be plotted in length graph).",
    required=True,
)
parser.add_argument(
    "--max_length",
    metavar="NNN",
    type=int,
    help="Maximum limit of expected amplicon length (will be plotted in length graph).",
    required=True,
)
parser.add_argument(
    "--add_taxonomy",
    help="Has taxonomy been added to the OTU and ESV tables? Information required for krona graphs.",
    choices=["True", "False"],
)
parser.add_argument(
    "--make_maps",
    help="Should GBIF-based maps be generated to infer species distribution?",
    default="True",
    choices=["True", "False"],
)
parser.add_argument(
    "--database_format",
    help="Format of the reference database. Information required for krona graphs.",
    choices=["midori2", "pr2", "silva", "bold"],
)
parser.add_argument(
    "--remove_negative_controls",
    help="Have negatve controls been removed? Information required for krona graphs.",
    choices=["True", "False"],
)
parser.add_argument(
    "--scaling_factor",
    help="Scaling factor for graph width. Manual trial and error in 0.2 increments might be required (default: 1).",
    default=1,
    type=float,
)

# Register the custom validation function to be called after parsing arguments
parser.set_defaults(func=validate_args)

# Parse arguments
args = parser.parse_args()

# Call the custom validation function to check the requirements
args.func(args)

# Set arguments
project_dir = args.project_dir
graph_format = args.graph_format
min_length = args.min_length
max_length = args.max_length
scaling_factor = args.scaling_factor
add_taxonomy = args.add_taxonomy
make_maps = args.make_maps
if add_taxonomy == "True":
    database_format = args.database_format
project_name = os.path.basename(project_dir)
if args.remove_negative_controls == "True":
    microdecon_suffix = "_microdecon-filtered"
else:
    microdecon_suffix = ""

############################ Start of pipeline
time_print("Generating apscale processing graphs...")

# Make outdir for project_dir if it doesn't already exist
outdir = os.path.join(project_dir, "0_statistics_and_graphs")
os.makedirs(outdir, exist_ok=True)
if make_maps == "True":
    mapdir = os.path.join(outdir, "GBIF_species_occurrence_maps")
    os.makedirs(mapdir, exist_ok=True)


# Import files
time_print("Importing files. This can take a while...")
report_file = os.path.join(project_dir, "Project_report.xlsx")
esv_postlulu_file = os.path.join(
    project_dir,
    "9_lulu_filtering",
    "denoising",
    f"{project_name}_ESV_table_filtered.parquet.snappy",
)
esv_prelulu_file = os.path.join(
    project_dir, "8_denoising", f"{project_name}_ESV_table.parquet.snappy"
)
otu_postlulu_file = os.path.join(
    project_dir,
    "9_lulu_filtering",
    "otu_clustering",
    f"{project_name}_OTU_table_filtered.parquet.snappy",
)
otu_prelulu_file = os.path.join(
    project_dir,
    "7_otu_clustering",
    f"{project_name}_OTU_table.parquet.snappy",
)

report_sheet_dict = pd.read_excel(report_file, sheet_name=None)
esv_postlulu_df = pd.read_parquet(esv_postlulu_file, engine="fastparquet")
time_print("1/4 files imported...")
esv_prelulu_df = pd.read_parquet(esv_prelulu_file, engine="fastparquet")
time_print("2/4 files imported...")
otu_postlulu_df = pd.read_parquet(otu_postlulu_file, engine="fastparquet")
time_print("3/4 files imported...")
otu_prelulu_df = pd.read_parquet(otu_prelulu_file, engine="fastparquet")
time_print("4/4 files imported. Import done. Generating graphs...")

# ESV table processing
esv_postlulu_df_mod = esv_postlulu_df.drop("Seq", axis=1).set_index("ID")
esv_postlulu_df_mod[esv_postlulu_df_mod > 1] = 1
esv_postlulu_sums = esv_postlulu_df_mod.sum()
esv_postlulu_sums.name = "count"
esv_postlulu_sums.index = (
    esv_postlulu_sums.index.str.replace("_PE", "")
    .str.replace("_trimmed", "")
    .str.replace("_filtered", "")
    .str.replace("_dereplicated", "")
)
esv_postlulu_sums = esv_postlulu_sums.reset_index()

esv_prelulu_df = esv_prelulu_df.drop("Seq", axis=1).set_index("ID")
esv_prelulu_df[esv_prelulu_df > 1] = 1
esv_prelulu_sums = esv_prelulu_df.sum()
esv_prelulu_sums.name = "count"
esv_prelulu_sums.index = (
    esv_prelulu_sums.index.str.replace("_PE", "")
    .str.replace("_trimmed", "")
    .str.replace("_filtered", "")
    .str.replace("_dereplicated", "")
)
esv_prelulu_sums = esv_prelulu_sums.reset_index()

# OTU table processing
otu_postlulu_df_mod = otu_postlulu_df.drop("Seq", axis=1).set_index("ID")
otu_postlulu_df_mod[otu_postlulu_df_mod > 1] = 1
otu_postlulu_sums = otu_postlulu_df_mod.sum()
otu_postlulu_sums.name = "count"
otu_postlulu_sums.index = (
    otu_postlulu_sums.index.str.replace("_PE", "")
    .str.replace("_trimmed", "")
    .str.replace("_filtered", "")
    .str.replace("_dereplicated", "")
)
otu_postlulu_sums = otu_postlulu_sums.reset_index()

otu_prelulu_df = otu_prelulu_df.drop("Seq", axis=1).set_index("ID")
otu_prelulu_df[otu_prelulu_df > 1] = 1
otu_prelulu_sums = otu_prelulu_df.sum()
otu_prelulu_sums.name = "count"
otu_prelulu_sums.index = (
    otu_prelulu_sums.index.str.replace("_PE", "")
    .str.replace("_trimmed", "")
    .str.replace("_filtered", "")
    .str.replace("_dereplicated", "")
)
otu_prelulu_sums = otu_prelulu_sums.reset_index()

# Stats table processing
## Gather stats
samples = (
    report_sheet_dict["3_PE merging"]["File"].str.replace("_PE.fastq.gz", "").tolist()
)
raw_reads = report_sheet_dict["3_PE merging"]["processed reads"].values.tolist()
merged_reads = report_sheet_dict["3_PE merging"]["merged reads"].values.tolist()
trimmed_reads = report_sheet_dict["4_primer_trimming"]["trimmed reads"].values.tolist()
filtered_reads = report_sheet_dict["5_quality_filtering"][
    "passed reads"
].values.tolist()
mapped_reads_OTUs = [sum(otu_postlulu_df[sample].values.tolist()) for sample in samples]
mapped_reads_ESVs = [sum(esv_postlulu_df[sample].values.tolist()) for sample in samples]
## Make stats df
df_stats = pd.DataFrame()
df_stats["Raw reads"] = raw_reads + calculate_read_stats(raw_reads)
df_stats["Merged reads"] = merged_reads + calculate_read_stats(merged_reads)
df_stats["Trimmed reads"] = trimmed_reads + calculate_read_stats(trimmed_reads)
df_stats["Filtered reads"] = filtered_reads + calculate_read_stats(filtered_reads)
df_stats["Mapped reads (OTUs)"] = mapped_reads_OTUs + calculate_read_stats(
    mapped_reads_OTUs
)
df_stats["Mapped reads (ESVs)"] = mapped_reads_ESVs + calculate_read_stats(
    mapped_reads_ESVs
)
df_stats.index = samples + [
    "_minimum",
    "_maximum",
    "_average",
    "_median",
    "_deviation",
]
## Save stats df
out_xlsx = os.path.join(
    outdir,
    f"{project_name}_summary_stats.xlsx",
)
df_stats.to_excel(out_xlsx)

# Graphs
# Set graph width based on number of samples (note: doesn't work consistently)
graph_width = 400 + len(report_sheet_dict["3_PE merging"]) * 25 * scaling_factor

# PE merging
perc_kept_pe = pd.DataFrame(
    {
        "value": report_sheet_dict["3_PE merging"]["merged reads"]
        / report_sheet_dict["3_PE merging"]["processed reads"]
        * 100
    }
)
perc_kept_pe["sample"] = (
    report_sheet_dict["3_PE merging"]["File"]
    .str.replace("_PE", "")
    .str.replace(".fastq.gz", "")
)  # type: ignore
perc_kept_pe = perc_kept_pe.fillna(0)
pe_graph = px.bar(
    perc_kept_pe.sort_values("value"),
    x="sample",
    y="value",
    range_y=[0, 100],
    labels={"value": "Reads kept [%]", "sample": "Sample"},
    title="Percentage of reads kept during PE merging",
    width=graph_width,
    # hover_name="sample",
    # hover_data=["value"],
)
pe_graph.add_hline(
    y=(sum(perc_kept_pe["value"]) / len(perc_kept_pe)),
    line_width=1,
    line_dash="dash",
)
pe_graph.update_layout(showlegend=False)
pe_graph.update_xaxes(tickangle=55)
## Save
if graph_format == "html":
    pe_graph.write_html(
        os.path.join(
            outdir,
            f"{project_name}_1_pe_merging.{graph_format}",
        )
    )
else:
    pe_graph.write_image(
        os.path.join(
            outdir,
            f"{project_name}_1_pe_merging.{graph_format}",
        )
    )
time_print("PE graph generated.")

# Primer trimming
perc_kept_trim = pd.DataFrame(
    {
        "value": report_sheet_dict["4_primer_trimming"]["trimmed reads"]
        / report_sheet_dict["4_primer_trimming"]["processed reads"]
        * 100
    }
)
perc_kept_trim["sample"] = (
    report_sheet_dict["4_primer_trimming"]["File"]
    .str.replace("_PE", "")
    .str.replace("_trimmed", "")
    .str.replace(".fastq.gz", "")
)  # type: ignore
perc_kept_trim = perc_kept_trim.fillna(0)
trim_graph = px.bar(
    perc_kept_trim.sort_values("value"),
    x="sample",
    y="value",
    range_y=[0, 100],
    labels={"value": "Reads kept [%]", "sample": "Samples"},
    title="Percentage of reads kept during trimming",
    width=graph_width,
)
trim_graph.add_hline(
    y=(sum(perc_kept_trim["value"]) / len(perc_kept_trim)),
    line_width=1,
    line_dash="dash",
)
trim_graph.update_layout(showlegend=False)
trim_graph.update_xaxes(tickangle=55)
# Save
if graph_format == "html":
    trim_graph.write_html(
        os.path.join(
            outdir,
            f"{project_name}_2_trimming.{graph_format}",
        )
    )
else:
    trim_graph.write_image(
        os.path.join(
            outdir,
            f"{project_name}_2_trimming.{graph_format}",
        )
    )
time_print("Trimming graph generated.")

# Read length distribution
time_print(
    "Reading in sequence files to generate read lengths graph. This can take a while..."
)
trimmed_seqs_dir = os.path.join(
    project_dir,
    "4_primer_trimming",
    "data",
)
# Make list of all samples
trimmed_seqs_files = [
    os.path.join(trimmed_seqs_dir, sample) for sample in os.listdir(trimmed_seqs_dir)
]

# Use the commandline to generate read length distribution file
## Construct the command
readlength_command = [
    "gunzip",
    "-c",
    *trimmed_seqs_files,
    "|",
    "awk",
    "'NR%4 == 2 {lengths[length($0)]++} END {for (l in lengths) {print l, lengths[l]}}'",
]
readlength_command = " ".join(readlength_command)

# Run the command and capture the output
readlength_result = subprocess.run(
    readlength_command, shell=True, stdout=subprocess.PIPE, text=True
)

# Convert the stdout to a Pandas DataFrame
readlength_result_lines = readlength_result.stdout.strip().split("\n")
readlength_result_data = [line.split() for line in readlength_result_lines]
readlength_df = pd.DataFrame(readlength_result_data, columns=["ReadLength", "Count"])
readlength_df = readlength_df.astype(int)

# Plot
length_graph = px.bar(
    readlength_df,
    x="ReadLength",
    y="Count",
    labels={"ReadLength": "Sequence length", "Count": "Frequency"},
    title="PE-merged read lengths before quality filtering",
)
length_graph.add_vline(
    x=min_length,
    line_width=1,
    line_dash="dash",
)
length_graph.add_vline(
    x=max_length,
    line_width=1,
    line_dash="dash",
)

# Save
if graph_format == "html":
    length_graph.write_html(
        os.path.join(
            outdir,
            f"{project_name}_3_read_lengths.{graph_format}",
        )
    )
else:
    length_graph.write_image(
        os.path.join(
            outdir,
            f"{project_name}_3_read_lengths.{graph_format}",
        )
    )
time_print("Read lengths graph generated.")

# Quality filtering
perc_kept_qf = pd.DataFrame(
    {
        "value": report_sheet_dict["5_quality_filtering"]["passed reads"]
        / report_sheet_dict["5_quality_filtering"]["processed reads"]
        * 100
    }
)
perc_kept_qf["sample"] = (
    report_sheet_dict["5_quality_filtering"]["File"]
    .str.replace("_PE", "")
    .str.replace("_trimmed", "")
    .str.replace("_filtered", "")
    .str.replace(".fasta.gz", "")
)  # type: ignore
perc_kept_qf = perc_kept_qf.fillna(0)
qf_graph = px.bar(
    perc_kept_qf.sort_values("value"),
    x="sample",
    y="value",
    range_y=[0, 100],
    labels={"value": "Reads kept [%]", "sample": "Sample"},
    title="Percentage of reads kept during quality filtering",
    width=graph_width,
)
qf_graph.add_hline(
    y=(sum(perc_kept_qf["value"]) / len(perc_kept_qf)),
    line_width=1,
    line_dash="dash",
)
qf_graph.update_layout(showlegend=False)
qf_graph.update_xaxes(tickangle=55)
# Save
if graph_format == "html":
    qf_graph.write_html(
        os.path.join(
            outdir,
            f"{project_name}_4_qualityFiltering.{graph_format}",
        )
    )
else:
    qf_graph.write_image(
        os.path.join(
            outdir,
            f"{project_name}_4_qualityFiltering.{graph_format}",
        )
    )
time_print("Quality filtering graph generated.")

# Dereplication
perc_kept_derep = pd.DataFrame(
    {
        "value": report_sheet_dict["6_dereplication"]["unique sequences"]
        / report_sheet_dict["6_dereplication"]["processed sequences"]
        * 100
    }
)
perc_kept_derep["sample"] = (
    report_sheet_dict["6_dereplication"]["File"]
    .str.replace("_PE", "")
    .str.replace("_trimmed", "")
    .str.replace("_filtered", "")
    .str.replace("_dereplicated", "")
    .str.replace(".fasta.gz", "")
)  # type: ignore
perc_kept_derep = perc_kept_derep.fillna(0)
derep_graph = px.bar(
    perc_kept_derep.sort_values("value"),
    x="sample",
    y="value",
    range_y=[0, 100],
    labels={"value": "Unique reads [%]", "sample": "Sample"},
    title="Number of unique reads per sample",
    width=graph_width,
)
derep_graph.add_hline(
    y=(sum(perc_kept_derep["value"]) / len(perc_kept_derep)),
    line_width=1,
    line_dash="dash",
)
derep_graph.update_layout(showlegend=False)
derep_graph.update_xaxes(tickangle=55)
# Save
if graph_format == "html":
    derep_graph.write_html(
        os.path.join(
            outdir,
            f"{project_name}_5_dereplication.{graph_format}",
        )
    )
else:
    derep_graph.write_image(
        os.path.join(
            outdir,
            f"{project_name}_5_dereplication.{graph_format}",
        )
    )
time_print("Dereplication graph generated.")

# Raw number of reads
num_reads_raw = pd.DataFrame(
    {"value": report_sheet_dict["3_PE merging"]["processed reads"]}
)
num_reads_raw["sample"] = (
    report_sheet_dict["3_PE merging"]["File"]
    .str.replace("_PE.fastq.gz", "")
    .str.replace(".fastq.gz", "")
)  # type: ignore
ymax_reads = max(num_reads_raw["value"])
rawreads_graph = px.bar(
    num_reads_raw.sort_values("value"),
    labels={"value": "Number of reads", "sample": "Sample"},
    title="Number of reads before PE merging, trimming and quality filtering",
    range_y=[0, ymax_reads],
    x="sample",
    y="value",
    width=graph_width,
)
rawreads_graph.add_hline(
    y=(sum(num_reads_raw["value"]) / len(num_reads_raw)), line_width=1, line_dash="dash"
)
rawreads_graph.update_layout(showlegend=False)
rawreads_graph.update_xaxes(tickangle=55)
# Save
if graph_format == "html":
    rawreads_graph.write_html(
        os.path.join(
            outdir,
            f"{project_name}_6_numberRawreads.{graph_format}",
        )
    )
else:
    rawreads_graph.write_image(
        os.path.join(
            outdir,
            f"{project_name}_6_numberRawreads.{graph_format}",
        )
    )
time_print("Raw reads graph generated.")

# Number of reads after PE merging and quality filtering
num_reads_filtered = pd.DataFrame(
    {"value": report_sheet_dict["5_quality_filtering"]["passed reads"]}
)
num_reads_filtered["sample"] = (
    report_sheet_dict["5_quality_filtering"]["File"]
    .str.replace("_PE", "")
    .str.replace("_trimmed", "")
    .str.replace("_filtered", "")
    .str.replace(".fasta.gz", "")
)  # type: ignore
filteredreads_graph = px.bar(
    num_reads_filtered.sort_values("value"),
    labels={"value": "Number of reads", "sample": "Sample"},
    title="Number of reads after PE merging, trimming and quality filtering",
    range_y=[0, ymax_reads],
    width=graph_width,
    x="sample",
    y="value",
)
filteredreads_graph.add_hline(
    y=(sum(num_reads_filtered["value"]) / len(num_reads_filtered)),
    line_width=1,
    line_dash="dash",
)
filteredreads_graph.update_layout(showlegend=False)
filteredreads_graph.update_xaxes(tickangle=55)
# Save
if graph_format == "html":
    filteredreads_graph.write_html(
        os.path.join(
            outdir,
            f"{project_name}_7_numberFilteredreads.{graph_format}",
        )
    )
else:
    filteredreads_graph.write_image(
        os.path.join(
            outdir,
            f"{project_name}_7_numberFilteredreads.{graph_format}",
        )
    )
time_print("Filtered reads graph generated.")

# Number of ESVs per sample before LULU
ymax_esvs = esv_prelulu_sums["count"].max()
prelulu_graph_esvs = px.bar(
    esv_prelulu_sums.sort_values("count"),
    labels={"count": "Number of ESVs", "index": "Sample"},
    title="Number of ESVs per sample before LULU filtering",
    range_y=[0, ymax_esvs],
    width=graph_width,
    y="count",
    x="index",
)
prelulu_graph_esvs.add_hline(
    y=(sum(esv_prelulu_sums["count"]) / len(esv_prelulu_sums)),
    line_width=1,
    line_dash="dash",
)
prelulu_graph_esvs.update_layout(showlegend=False)
prelulu_graph_esvs.update_xaxes(tickangle=55)
# Save
if graph_format == "html":
    prelulu_graph_esvs.write_html(
        os.path.join(
            outdir,
            f"{project_name}_8_prelulu_esvs.{graph_format}",
        )
    )
else:
    prelulu_graph_esvs.write_image(
        os.path.join(
            outdir,
            f"{project_name}_8_prelulu_esvs.{graph_format}",
        )
    )
time_print("Number of ESVs before LULU graph generated.")

# Number of ESVs per sample after LULU
postlulu_graph_esvs = px.bar(
    esv_postlulu_sums.sort_values("count"),
    labels={"count": "Number of ESVs", "index": "Sample"},
    title="Number of ESVs per sample after LULU filtering",
    range_y=[0, ymax_esvs],
    width=graph_width,
    y="count",
    x="index",
)
postlulu_graph_esvs.add_hline(
    y=(sum(esv_postlulu_sums["count"]) / len(esv_postlulu_sums)),
    line_width=1,
    line_dash="dash",
)
postlulu_graph_esvs.update_layout(showlegend=False)
postlulu_graph_esvs.update_xaxes(tickangle=55)
# Save
if graph_format == "html":
    postlulu_graph_esvs.write_html(
        os.path.join(
            outdir,
            f"{project_name}_9_postlulu_esvs.{graph_format}",
        )
    )
else:
    postlulu_graph_esvs.write_image(
        os.path.join(
            outdir,
            f"{project_name}_9_postlulu_esvs.{graph_format}",
        )
    )
time_print("Number of ESVs after LULU graph generated.")

# Number of OTUs per sample before LULU
ymax_otus = otu_prelulu_sums["count"].max()
prelulu_graph_otus = px.bar(
    otu_prelulu_sums.sort_values("count"),
    labels={"count": "Number of OTUs", "index": "Sample"},
    title="Number of OTUs per sample before LULU filtering",
    range_y=[0, ymax_otus],
    width=graph_width,
    y="count",
    x="index",
)
prelulu_graph_otus.add_hline(
    y=(sum(otu_prelulu_sums["count"]) / len(otu_prelulu_sums)),
    line_width=1,
    line_dash="dash",
)
prelulu_graph_otus.update_layout(showlegend=False)
prelulu_graph_otus.update_xaxes(tickangle=55)
# Save
if graph_format == "html":
    prelulu_graph_otus.write_html(
        os.path.join(
            outdir,
            f"{project_name}_10_prelulu_otus.{graph_format}",
        )
    )
else:
    prelulu_graph_otus.write_image(
        os.path.join(
            outdir,
            f"{project_name}_10_prelulu_otus.{graph_format}",
        )
    )
time_print("Number of OTUs before LULU graph generated.")

# Number of OTUs per sample after LULU
postlulu_graph_otus = px.bar(
    otu_postlulu_sums.sort_values("count"),
    labels={"count": "Number of OTUs", "index": "Sample"},
    title="Number of OTUs per sample after LULU filtering",
    range_y=[0, ymax_otus],
    width=graph_width,
    y="count",
    x="index",
)
postlulu_graph_otus.add_hline(
    y=(sum(otu_postlulu_sums["count"]) / len(otu_postlulu_sums)),
    line_width=1,
    line_dash="dash",
)
postlulu_graph_otus.update_layout(showlegend=False)
postlulu_graph_otus.update_xaxes(tickangle=55)
# Save
if graph_format == "html":
    postlulu_graph_otus.write_html(
        os.path.join(
            outdir,
            f"{project_name}_11_postlulu_otus.{graph_format}",
        )
    )
else:
    postlulu_graph_otus.write_image(
        os.path.join(
            outdir,
            f"{project_name}_11_postlulu_otus.{graph_format}",
        )
    )
time_print("Number of OTUs after LULU graph generated.")

# LULU pre post overview
ESVs = len(esv_prelulu_df)
ESVs_filtered = len(esv_postlulu_df)
OTUs = len(otu_prelulu_df)
OTUs_filtered = len(otu_postlulu_df)
lulu_pre_post_graph = go.Figure()
x_values = ["ESVs", "ESVs LULU-filtered", "OTUs", "OTUs LULU-filtered"]
y_values = [ESVs, ESVs_filtered, OTUs, OTUs_filtered]
text = [ESVs, ESVs_filtered, OTUs, OTUs_filtered]
lulu_pre_post_graph.add_trace(go.Bar(x=x_values, y=y_values, text=text))
lulu_pre_post_graph.update_layout(
    width=1000,
    height=800,
    title="Total number of ESVs and OTUs before and after LULU filtering",
)
lulu_pre_post_graph.update_traces(textposition="outside")
lulu_pre_post_graph.update_yaxes(title="OTUs/ESVs")
# Save
if graph_format == "html":
    lulu_pre_post_graph.write_html(
        os.path.join(
            outdir,
            f"{project_name}_12_lulu_filtering_stats.{graph_format}",
        )
    )
else:
    lulu_pre_post_graph.write_image(
        os.path.join(
            outdir,
            f"{project_name}_12_lulu_filtering_stats.{graph_format}",
        )
    )
time_print("Pre- vs. post-LULU comparison graph generated.")

# Number of reads vs number of ESVs
reads_esvs_graph = px.scatter(
    pd.concat([num_reads_filtered, esv_postlulu_sums["count"]], axis=1).reset_index(),
    x="value",
    y="count",
    trendline="ols",
    trendline_color_override="black",
    hover_name="sample",
    title="Filtered reads vs. ESVs after LULU filtering",
    width=480,
    labels={
        "count": "Number of ESVs after LULU filtering",
        "value": "Number of filtered reads",
    },
)
# Save
if graph_format == "html":
    reads_esvs_graph.write_html(
        os.path.join(
            outdir,
            f"{project_name}_13_reads_vs_esvs.{graph_format}",
        )
    )
else:
    reads_esvs_graph.write_image(
        os.path.join(
            outdir,
            f"{project_name}_13_reads_vs_esvs.{graph_format}",
        )
    )
time_print("Number of reads vs. ESVs graph generated.")


# Number of reads vs number of OTUs
reads_otus_graph = px.scatter(
    pd.concat([num_reads_filtered, otu_postlulu_sums["count"]], axis=1).reset_index(),
    x="value",
    y="count",
    trendline="ols",
    trendline_color_override="black",
    hover_name="sample",
    title="Filtered reads vs. OTUs after LULU filtering",
    width=480,
    labels={
        "count": "Number of OTUs after LULU filtering",
        "value": "Number of filtered reads",
    },
)
# Save
if graph_format == "html":
    reads_otus_graph.write_html(
        os.path.join(
            outdir,
            f"{project_name}_14_reads_vs_otus.{graph_format}",
        )
    )
else:
    reads_otus_graph.write_image(
        os.path.join(
            outdir,
            f"{project_name}_14_reads_vs_otus.{graph_format}",
        )
    )
time_print("Number of reads vs. OTUs graph generated.")

# Add Accumulation curves - ESVs, OTUs, ESVs species, OTUs species


# Lineplot ESVs
esv_linegraph = go.Figure()
for sample in samples:
    y_values = df_stats.loc[sample].values.tolist()[:-2] + [
        df_stats.loc[sample].values.tolist()[-1]
    ]
    x_values = df_stats.columns.tolist()[:-2] + [df_stats.columns.tolist()[-1]]
    esv_linegraph.add_trace(go.Scatter(x=x_values, y=y_values, name=sample))
esv_linegraph.update_layout(
    template="simple_white",
    width=1000,
    height=800,
    title="ESVs - Reads per sample for each processing step",
)
esv_linegraph.update_yaxes(title="Reads")
esv_linegraph = esv_linegraph.update_layout(showlegend=False)
# Save
if graph_format == "html":
    esv_linegraph.write_html(
        os.path.join(
            outdir,
            f"{project_name}_15_linegraph_esvs.{graph_format}",
        )
    )
else:
    esv_linegraph.write_image(
        os.path.join(
            outdir,
            f"{project_name}_15_linegraph_esvs.{graph_format}",
        )
    )
time_print("Lineplot ESVs graph generated.")

# Lineplot OTUs
otu_linegraph = go.Figure()
for sample in samples:
    y_values = df_stats.loc[sample].values.tolist()[:-1]
    x_values = df_stats.columns.tolist()[:-1]
    otu_linegraph.add_trace(go.Scatter(x=x_values, y=y_values, name=sample))
otu_linegraph.update_layout(
    template="simple_white",
    width=1000,
    height=800,
    title="OTUs - Reads per sample for each processing step",
)
otu_linegraph.update_yaxes(title="Reads")
otu_linegraph = otu_linegraph.update_layout(showlegend=False)
# Save
if graph_format == "html":
    otu_linegraph.write_html(
        os.path.join(
            outdir,
            f"{project_name}_16_linegraph_otus.{graph_format}",
        )
    )
else:
    otu_linegraph.write_image(
        os.path.join(
            outdir,
            f"{project_name}_16_linegraph_otus.{graph_format}",
        )
    )
time_print("Lineplot OTUs graph generated.")

# Boxplot
boxplot = go.Figure()
for category in df_stats.columns.tolist():
    y_values = df_stats.loc[samples][category].values.tolist()
    boxplot.add_trace(go.Box(y=y_values, name=category))
boxplot.update_layout(
    width=1000,
    height=800,
    title="Read number overview for each processing step",
    showlegend=False,
)
boxplot.update_yaxes(title="Reads")
# Save
if graph_format == "html":
    boxplot.write_html(
        os.path.join(
            outdir,
            f"{project_name}_17_boxplot_read_summary.{graph_format}",
        )
    )
else:
    boxplot.write_image(
        os.path.join(
            outdir,
            f"{project_name}_17_boxplot_read_summary.{graph_format}",
        )
    )
time_print("Boxplot generated.")

# ESV clustergram
## Format
ID_list_esv = esv_postlulu_df["ID"].values.tolist()
esv_postlulu_samples_df = esv_postlulu_df.drop(columns=["ID", "Seq"])
sample_list = esv_postlulu_samples_df.columns.tolist()
## Take log
esv_postlulu_samples_log_df = np.where(
    esv_postlulu_samples_df != 0, np.log(esv_postlulu_samples_df), 0
)
## Define colours
colors = px.colors.sample_colorscale("plasma", [n / 5 for n in range(5)])
## Create clustergram
time_print("Generating clustergram for ESVs. This can take a while...")
esv_clustergram = dash_bio.Clustergram(
    data=esv_postlulu_samples_log_df,
    column_labels=list(esv_postlulu_samples_df.columns.values),
    row_labels=list(esv_postlulu_samples_df.index),
    height=800,
    width=min(graph_width, 3000),
    hidden_labels="row",
    color_map=[
        [0.0, colors[0]],
        [0.25, colors[1]],
        [0.5, colors[2]],
        [0.75, colors[3]],
        [1.0, colors[4]],
    ],
    paper_bg_color="white",
)
esv_clustergram.update_layout(
    title="Clustergram with log-transformed ESV abundances",
)
# Save
if graph_format == "html":
    esv_clustergram.write_html(
        os.path.join(
            outdir,
            f"{project_name}_18_esv_clustergram.{graph_format}",
        )
    )
else:
    esv_clustergram.write_image(
        os.path.join(
            outdir,
            f"{project_name}_18_esv_clustergram.{graph_format}",
        )
    )
time_print("Clustergram generated for ESVs.")

# OTU clustergram
## Format
ID_list_otu = otu_postlulu_df["ID"].values.tolist()
otu_postlulu_samples_df = otu_postlulu_df.drop(columns=["ID", "Seq"])
sample_list = otu_postlulu_samples_df.columns.tolist()
## Take log
otu_postlulu_samples_log_df = np.where(
    otu_postlulu_samples_df != 0, np.log(otu_postlulu_samples_df), 0
)
## Define colours
colors = px.colors.sample_colorscale("plasma", [n / 5 for n in range(5)])
## Create clustergram
time_print("Generating clustergram for OTUs. This can take a while...")
otu_clustergram = dash_bio.Clustergram(
    data=otu_postlulu_samples_log_df,
    column_labels=list(otu_postlulu_samples_df.columns.values),
    row_labels=list(otu_postlulu_samples_df.index),
    height=800,
    width=min(graph_width, 3000),
    hidden_labels="row",
    color_map=[
        [0.0, colors[0]],
        [0.25, colors[1]],
        [0.5, colors[2]],
        [0.75, colors[3]],
        [1.0, colors[4]],
    ],
    paper_bg_color="white",
)
otu_clustergram.update_layout(
    title="Clustergram with log-transformed OTU abundances",
)
## Save
if graph_format == "html":
    otu_clustergram.write_html(
        os.path.join(
            outdir,
            f"{project_name}_19_otu_clustergram.{graph_format}",
        )
    )
else:
    otu_clustergram.write_image(
        os.path.join(
            outdir,
            f"{project_name}_19_otu_clustergram.{graph_format}",
        )
    )
time_print("Clustergram generated for OTUs.")

# Kronagraphs
if (
    add_taxonomy == "True"
):  # Requirement as we need taxonomic information for Kronagraphs
    time_print("Generating kronagraphs...")

    time_print("Importing final ESV and OTU tables...")
    otu_final_file = os.path.join(
        project_dir,
        "9_lulu_filtering",
        "otu_clustering",
        f"{project_name}_OTU_table_filtered{microdecon_suffix}_with_taxonomy.csv",
    )
    esv_final_file = os.path.join(
        project_dir,
        "9_lulu_filtering",
        "denoising",
        f"{project_name}_ESV_table_filtered{microdecon_suffix}_with_taxonomy.csv",
    )

    otu_final_df = pd.read_csv(otu_final_file)
    time_print("1/2 final files imported...")
    esv_final_df = pd.read_csv(esv_final_file)
    time_print("2/2 final files imported. Import done. Generating kronagraphs...")

    # Format dfs for Krona
    esv_krona_df = krona_formatting(esv_final_df)
    otu_krona_df = krona_formatting(otu_final_df)
    # Save so that krona can be run in command line
    esv_krona_df.to_csv(
        os.path.join(
            outdir,
            f"{project_name}_ESVs_krona-formatted.csv",
        ),
        header=False,
        index=False,
        sep="\t",
    )
    otu_krona_df.to_csv(
        os.path.join(
            outdir,
            f"{project_name}_OTUs_krona-formatted.csv",
        ),
        header=False,
        index=False,
        sep="\t",
    )
    # Use the commandline to run krona on both
    ## Construct the krona command for ESVs
    krona_command_esvs = " ".join(
        [
            "ktImportText",
            "-o",
            os.path.join(outdir, f"{project_name}_20_esv_krona.html"),
            os.path.join(outdir, f"{project_name}_ESVs_krona-formatted.csv"),
        ]
    )
    ## Run the command
    subprocess.call(krona_command_esvs, shell=True)
    ## Construct the krona command for OTUs
    krona_command_otus = " ".join(
        [
            "ktImportText",
            "-o",
            os.path.join(outdir, f"{project_name}_21_otu_krona.html"),
            os.path.join(outdir, f"{project_name}_OTUs_krona-formatted.csv"),
        ]
    )
    ## Run the command
    subprocess.call(krona_command_otus, shell=True)
    # Remove formatted files
    os.remove(os.path.join(outdir, f"{project_name}_ESVs_krona-formatted.csv"))
    os.remove(os.path.join(outdir, f"{project_name}_OTUs_krona-formatted.csv"))


# Maps
if make_maps == "True":
    time_print("Generating maps...")

    if add_taxonomy == "False":
        # Importing files
        otu_final_file = os.path.join(
            project_dir,
            "9_lulu_filtering",
            "otu_clustering",
            f"{project_name}_OTU_table_filtered{microdecon_suffix}_with_taxonomy.csv",
        )
        esv_final_file = os.path.join(
            project_dir,
            "9_lulu_filtering",
            "denoising",
            f"{project_name}_ESV_table_filtered{microdecon_suffix}_with_taxonomy.csv",
        )

        otu_final_df = pd.read_csv(otu_final_file)
        time_print("1/2 final files imported...")
        esv_final_df = pd.read_csv(esv_final_file)
        time_print("2/2 final files imported. Import done.")

    time_print("Standardizing species names with GBIF...")
    gbif_standardized_species_esvs = gbif_check_taxonomy(esv_final_df, "ESV")
    gbif_standardized_species_otus = gbif_check_taxonomy(otu_final_df, "OTU")

    (
        species_maps_esvs,
        continent_occurrence_plot_esvs,
        realm_occurrence_plot_esvs,
    ) = maps_and_continent_plot_generation(gbif_standardized_species_esvs, "ESV")
    species_maps_otus, continent_occurrence_plot_otus, realm_occurrence_plot_otus = (
        maps_and_continent_plot_generation(gbif_standardized_species_otus, "OTU")
    )

    ## Save
    if continent_occurrence_plot_otus:
        if graph_format == "html":
            for species in species_maps_otus:
                species_maps_otus[species].write_html(
                    os.path.join(
                        mapdir,
                        f"{species}.{graph_format}",
                    )
                )
            continent_occurrence_plot_otus.write_html(
                os.path.join(
                    outdir,
                    f"{project_name}_22_continent_occurrence_plot_otus.{graph_format}",
                )
            )
            realm_occurrence_plot_otus.write_html(
                os.path.join(
                    outdir,
                    f"{project_name}_23_realm_occurrence_plot_otus.{graph_format}",
                )
            )
        else:
            for species in species_maps_otus:
                species_maps_otus[species].write_image(
                    os.path.join(
                        mapdir,
                        f"{species}.{graph_format}",
                    )
                )
            continent_occurrence_plot_otus.write_image(
                os.path.join(
                    outdir,
                    f"{project_name}_22_continent_occurrence_plot_otus.{graph_format}",
                )
            )
            realm_occurrence_plot_otus.write_image(
                os.path.join(
                    outdir,
                    f"{project_name}_23_realm_occurrence_plot_otus.{graph_format}",
                )
            )

        time_print(
            "GBIF maps, continent occurrence plot, and realm occurrence plot generated for OTUs."
        )

    if continent_occurrence_plot_esvs:
        if graph_format == "html":
            for species in species_maps_esvs:
                species_maps_esvs[species].write_html(
                    os.path.join(
                        mapdir,
                        f"{species}.{graph_format}",
                    )
                )
            continent_occurrence_plot_esvs.write_html(
                os.path.join(
                    outdir,
                    f"{project_name}_24_continent_occurrence_plot_esvs.{graph_format}",
                )
            )
            realm_occurrence_plot_esvs.write_html(
                os.path.join(
                    outdir,
                    f"{project_name}_25_realm_occurrence_plot_esvs.{graph_format}",
                )
            )

        else:
            for species in species_maps_esvs:
                species_maps_esvs[species].write_image(
                    os.path.join(
                        mapdir,
                        f"{species}.{graph_format}",
                    )
                )
            continent_occurrence_plot_esvs.write_image(
                os.path.join(
                    outdir,
                    f"{project_name}_24_continent_occurrence_plot_esvs.{graph_format}",
                )
            )
            realm_occurrence_plot_esvs.write_image(
                os.path.join(
                    outdir,
                    f"{project_name}_25_realm_occurrence_plot_esvs.{graph_format}",
                )
            )

        time_print(
            "GBIF maps, continent occurrence plot, and realm occurrence plot generated for ESVs."
        )


time_print("Finished graph generation.")
